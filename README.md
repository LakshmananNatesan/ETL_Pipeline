# ETL_Pipeline
# ğŸ› ï¸ ETL Data Integration Pipeline

This project demonstrates a simple ETL (Extract, Transform, Load) pipeline using Python. It combines structured data from multiple file formats â€” CSV, JSON, and XML â€” transforms it, and exports a clean, consolidated dataset as a `.csv` file.

---

## ğŸ“‚ Project Structure
etl_project/
â”œâ”€â”€ extract.py # Extract logic for CSV, JSON, and XML
â”œâ”€â”€ transform.py # Data transformation logic
â”œâ”€â”€ load.py # Save output to CSV
â”œâ”€â”€ main.py # Main runner script
â”œâ”€â”€ data/ # Folder containing source files (CSV, JSON, XML)
â”œâ”€â”€ output/ # Folder where final_report.csv will be saved
â””â”€â”€ README.md # Project documentation



---

## ğŸ”§ Technologies Used

- Python 3.x
- pandas
- xml.etree.ElementTree
- glob
- OS module

---

## ğŸš€ How It Works

### 1. **Extract**
Reads all files from `data/` directory:
- Skips the final output file if it already exists
- Uses custom functions to extract data from:
  - `.csv` files using `pandas.read_csv()`
  - `.json` files using `pandas.read_json()`
  - `.xml` files using `ElementTree`

### 2. **Transform**
Applies the following transformations:
- Drops missing rows
- Converts height and weight to integers
- Capitalizes name strings

### 3. **Load**
Saves the final transformed dataset to:
